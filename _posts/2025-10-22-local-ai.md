---
title: "Eseguire modelli AI in locale (o in ambienti controllati): libert√†, limiti e prospettive"
date: 2025-10-22
excerpt: "Casi d'uso, soluzioni enterprise e customer... e alcune prove con AMD Strix Halo."
tags: [AI locale, LLM, aiopenweight, Strix Halo, generativeai]
author: alessio
classes: wide
header:
  overlay_image: /assets/images/local-ai/comfyui-krea-result.jpg
  overlay_filter: 0.7
---

## Eseguire modelli AI in locale (o in ambienti controllati): libert√†, limiti e prospettive

Mentre le news di nuovi servizi e funzionalit√† di Gemini, Claude, ChatGPT, etc. si susseguono, permane un certo interesse nella community a proposito di soluzioni per eseguire modelli di intelligenza artificiale generativa **fuori dai grandi servizi cloud**.
L'obbiettivo in sostanza √® far girare **modelli open-weight** ‚Äì cio√® con pesi pubblicamente scaricabili ‚Äì **sul proprio computer o in ambienti cloud ‚Äúcontrollati‚Äù**, dove si sceglie quale modello usare, con quali dati e con quali vincoli.

Ma perch√© dovremmo volerlo fare?

---

## Perch√© scegliere un ambiente controllato

L‚Äôuso di servizi gestiti (come ChatGPT o Gemini) offre comodit√† e prestazioni, ma comporta anche **limiti importanti**:

* **Controllo dei dati** ‚Äì i prompt, i file caricati e i risultati possono transitare su infrastrutture esterne, con implicazioni per la privacy o la propriet√† intellettuale.
* **Dipendenza dal provider** ‚Äì le API possono cambiare, i modelli essere aggiornati o rimossi senza preavviso.
* **Costi e scalabilit√†** ‚Äì per usi intensivi o sperimentali, le tariffe possono diventare elevate.
* **Limitazioni operative** ‚Äì molti servizi impongono limiti di contesto o non permettono l‚Äôesecuzione di modelli personalizzati.

Ecco perch√© si registra l‚Äôinteresse per soluzioni ‚Äú**local-first**‚Äù: esecuzione **in locale** o in **ambienti controllati**, cio√® **cloud indipendenti** o container privati dove si gestisce interamente l‚Äôinfrastruttura e i modelli.

---

## Local vs. ambiente controllato: cosa significa davvero

Parlare di ‚Äúlocale‚Äù non vuol dire per forza ‚Äúsul proprio portatile / desktop‚Äù.
Esistono oggi due grandi categorie:

1. **Esecuzione locale** ‚Äì tutto avviene sul proprio hardware, tipicamente con GPU o NPU dedicate.
2. **Esecuzione in ambienti controllati** ‚Äì si usano risorse cloud, ma mantenendo pieno controllo su modelli, codice e dati.

Alcuni servizi noti per quest‚Äôultimo approccio:

* **[RunPod](https://www.runpod.io/)** e **[TensorDock](https://tensordock.com/)** ‚Äì ambienti GPU ‚Äúpay-per-minute‚Äù per LLM e Stable Diffusion, con possibilit√† di personalizzare container e storage.
* **[RunComfy](https://www.runcomfy.com/)** ‚Äì piattaforma pronta per flussi *ComfyUI*, utile per generazione di immagini e video.
* **[Vast.ai](https://vast.ai/)** e **[Lambda Cloud](https://lambda.ai/service/gpu-cloud)** ‚Äì soluzioni per affittare GPU a breve termine, con immagini preconfigurate per PyTorch, Diffusers e llama.cpp.

Queste opzioni offrono un buon equilibrio tra libert√† e semplicit√†: il modello gira su macchine ‚Äútue‚Äù (o quasi), ma non devi preoccuparti di raffreddamento, alimentazione o driver.

---

## I limiti: prestazioni, qualit√†, contesto

Naturalmente, eseguire in locale o in ambienti controllati **non √® gratis** ‚Äî in termini di potenza e maturit√† del software:

* **Velocit√†**: un LLM da 70‚Äì120B pu√≤ raggiungere solo pochi token/s su GPU consumer.
* **Qualit√†**: i modelli open-weight pi√π accessibili (Mistral, LLaMA, Gemma, Qwen, Flux) tipicamente non eguagliano GPT-4 o Gemini 2.5.
* **Contesto limitato**: 4k‚Äì32k token tipici, contro i 128k‚Äì1M dei modelli gestiti.
* **Concorrenza**: in un setup locale si pu√≤ gestire solo un numero ridotto di richieste contemporanee.

Per questo, l‚Äôuso locale si presta bene a:

* **prototipazione e ricerca**,
* **automazioni personali**,
* **sperimentazione su dati sensibili**,
* **edge computing o applicazioni offline**.

A livello enterprise, invece, gli ambienti controllati permettono di costruire **pipeline AI interne**, con costi prevedibili e modelli verificabili.

---

## Eseguire modelli in locale: la questione hardware

E qui arriviamo al nodo centrale: **la memoria video (VRAM)**.
La VRAM determina la dimensione massima del modello che si pu√≤ caricare e la sua velocit√† di inferenza.

* Le **GPU consumer** (NVIDIA RTX 4090 o AMD 7900 XTX) arrivano a 24‚Äì32 GB di VRAM: gi√† sufficienti per modelli da 7‚Äì13B, ma non oltre.
* Oltre questa soglia servono **schede professionali** o **soluzioni con memoria condivisa**, che unificano RAM e VRAM.

Memoria condivisa: un cambio di paradigma

Negli ultimi anni la distinzione tra RAM di sistema e VRAM della GPU si sta assottigliando.
Nei sistemi tradizionali CPU e GPU hanno memorie fisiche separate: per elaborare un modello, i dati devono essere copiati avanti e indietro tra i due spazi, con un notevole costo in termini di banda e latenza.
Le nuove architetture invece puntano a unificare la memoria, permettendo a CPU e GPU di accedere allo stesso spazio fisico, in modo coerente e ad altissima velocit√†.
√à un cambiamento che semplifica il calcolo e apre scenari nuovi per l‚Äôesecuzione di modelli AI di grandi dimensioni, anche su macchine compatte.

Tre approcci oggi particolarmente interessanti:

üß© Apple Silicon (M3, M4, M4 Pro)

Apple √® stata la prima a portare su larga scala il concetto di Unified Memory Architecture (UMA): CPU, GPU e Neural Engine condividono la stessa memoria fisica, a banda elevata (fino a 120 GB/s).
Questo elimina la necessit√† di copiare i dati tra dispositivi e rende possibile caricare interamente un modello di grandi dimensioni (come un LLM da 13B) in un‚Äôunica area di memoria.
L‚Äôefficienza energetica √® eccellente, ma la memoria √® saldato-on-chip e quindi non espandibile; i tagli disponibili (fino a 48‚Äì64 GB nelle versioni Pro/Max) restano piuttosto costosi.

‚ö° NVIDIA DGX Spark (Grace Blackwell GB10)

Il DGX Spark, presentato al GTC 2025, rappresenta il primo sistema ‚Äúpersonale‚Äù della linea DGX.
Basato sul superchip NVIDIA GB10 Grace Blackwell, integra una CPU Grace a 20 core (10 Cortex-X925 + 10 Cortex-A725) e una GPU Blackwell di quinta generazione in un singolo package.
Le due unit√† condividono 128 GB di memoria LPDDR5X coerente tramite NVLink-C2C, che fornisce una banda fino a 5√ó superiore a PCIe Gen 5.
Il risultato √® un modello di memoria CPU+GPU realmente unificato, con accesso coerente e senza copia dei tensor, capace di gestire fino a 200 miliardi di parametri (o 405B collegando due sistemi via ConnectX a 200 Gbps).
Con un assorbimento di circa 240 W e dimensioni da mini-desktop (15 √ó 15 √ó 5 cm), il DGX Spark √® una sorta di ‚ÄúAI workstation da scrivania‚Äù, ponte diretto tra le architetture UMA di Apple e i supercluster Grace Hopper dei data center.

üî• AMD Ryzen‚Ñ¢ AI Max+ 395 (Strix Halo)

Strix Halo segue una logica simile, ma in ambito consumer e con architettura x86.
√à una APU monolitica che integra CPU Zen 5 e GPU RDNA 3.5, condividendo fino a 128 GB di memoria LPDDR5X con banda teorica di 250‚Äì300 GB/s.
Non ha VRAM dedicata: la GPU pu√≤ allocare dinamicamente porzioni della RAM di sistema, come avviene nei Mac.
Questo approccio consente di eseguire modelli di grandi dimensioni senza schede video discrete, con un ottimo compromesso tra prestazioni, costi e consumo energetico.

---

## La mia esperienza con AMD Ryzen‚Ñ¢ AI Max+ 395 (128 GB)

Sto sperimentando proprio questa piattaforma; esistono diverse soluzioni in commercio, io ho scelto [frame.work desktop](https://frame.work/it/en/desktop) nella versione con 128GB di memoria. Come sistema operativo, utilizzo unicamente Linux.

Uno dei vantaggi √® che, pur potendo configurare nel BIOS la quantit√† di memoria riservata alla GPU (ad esempio 512 MB), **Linux gestisce dinamicamente la condivisione RAM/VRAM**, come avviene nei sistemi Apple.
Per ottenere buone prestazioni √® necessario impostare alcuni parametri del kernel relativi all‚Äôallocazione (ad esempio `amdgpu.sg_display`, `amdgpu.vm_fragment_size`, `amdgpu.gtt_size`).

Il supporto software √® in piena evoluzione:

* **Kernel 6.18** introdurr√† correzioni importanti per la gestione della memoria su Strix Halo.
* **Driver Mesa** e **ROCm** hanno guadagnato fino al **+25 % di performance** negli ultimi mesi.
* Tuttavia, **instabilit√† e driver sperimentali** rendono l‚Äôesperienza ancora poco ‚Äúplug and play‚Äù.

Il bilancio complessivo √® positivo: **un rapporto costo/prestazioni molto interessante** per chi vuole sperimentare LLM e modelli generativi senza GPU dedicate.

---

## Toolbox e primi test pratici

Un ottimo punto di partenza sono le **[toolbox su GitHub di Donato Capitella](https://github.com/kyuz0)**, che semplificano l‚Äôesecuzione (ottimizzata per questo specifico hardware AMD) di *llama.cpp*, *Qwen Image Studio*, *Wan Video Studio* e *ComfyUI*.

Ecco ad esempio un benchmark di esecuzione di **GPT-OSS 120B** con *llama-bench*, con il backend *Vulkan* abilitato:

{% include figure image_path="/assets/images/local-ai/toolbox-gpt-oss-120b.jpg" alt="GPT-OSS 120B benchmark" caption="GPT-OSS 120B benchmark" %}

Interfaccia di generazione immagini **Qwen Image Studio**:

{% include figure image_path="/assets/images/local-ai/qwen-image-studio-generate.jpg" alt="Qwen Image Studio - generate" caption="Qwen Image Studio - generating an image" %}
{% include figure image_path="/assets/images/local-ai/qwen-image-studio-result.jpg" alt="Qwen Image Studio - result" caption="Qwen Image Studio - generated image" %}

E un workflow di generazione ibrido con **Flux Krea Dev** dentro *ComfyUI*:

{% include figure image_path="/assets/images/local-ai/comfyui-krea-generate.jpg" alt="ComfyUI running Flux Krea Dev workflow" caption="ComfyUI running Flux Krea Dev workflow" %}
{% include figure image_path="/assets/images/local-ai/comfyui-krea-result.jpg" alt="Flux Krea Dev workflow complete ini ComfyUI" caption="Flux Krea Dev workflow complete ini ComfyUI" %}

---

## Considerazioni sulle configurazioni

Un aspetto curioso su Strix Halo: spesso le **quantizzazioni BF16** risultano **pi√π efficienti di FP8**, nonostante l‚Äôapparente ‚Äúmaggiore precisione‚Äù.
Questo dipende dall‚Äôimplementazione dei driver ROCm e dal modo in cui viene gestita la memoria condivisa: le operazioni FP8, pur teoricamente pi√π leggere, introducono overhead nella conversione e nella gestione dei buffer.

---

## Conclusione e prospettive

Eseguire modelli AI in locale o in ambienti controllati significa **riappropriarsi del controllo** su dati, costi e infrastruttura.
Chiaramente non √® ancora un percorso lineare: servono competenze tecniche, pazienza e una certa tolleranza ai bug quando si ha a che fare con driver e librerie sperimentali.
Infine, in ambito consumer le soluzioni con memoria unificata e toolkit open-source consentono di ottenere risultati di discreta qualit√† in tempi ragionevoli.

Nei prossimi post racconter√≤ **alcuni use case concreti su Strix Halo**, con esempi di automazione locale e generazione di contenuti multimodali.

